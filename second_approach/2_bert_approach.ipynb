{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Named Entity Recognition with BERT",
   "id": "cf247b9293648fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=3)  # Adjust num_labels based on unique labels in your dataset\n",
    "model.to(DEVICE)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\" Load JSON data from a file. \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def create_labels(tokens, ners):\n",
    "    \"\"\" Create labels aligned with token positions in the document. \"\"\"\n",
    "    labels = [-100] * len(tokens)  # Use -100 to ignore certain positions in loss calculation\n",
    "    if ners:\n",
    "        for start, end, label in ners:\n",
    "            for i, token in enumerate(tokens):\n",
    "                token_start = token.offsets[0]\n",
    "                token_end = token.offsets[1] - 1\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    labels[i] = label\n",
    "    return labels\n",
    "\n",
    "def encode_data(data):\n",
    "    \"\"\" Encode texts and labels for NER training or prediction. \"\"\"\n",
    "    encoded_texts = []\n",
    "    encoded_labels = []\n",
    "    for item in data:\n",
    "        text_field = 'sentences' if 'sentences' in item else 'senences'\n",
    "        # Tokenize the text; ensure that the tokenizer returns offset mappings\n",
    "        encodings = tokenizer(item[text_field].split(), is_split_into_words=True, return_offsets_mapping=True)\n",
    "        words = item[text_field].split()\n",
    "        labels = [-100] * len(encodings['input_ids'])  # Use -100 to ignore certain positions in loss calculation\n",
    "        ners = item.get('ners', [])\n",
    "        idx = 0\n",
    "\n",
    "        # Align the labels with token offsets\n",
    "        for start, end, label in ners:\n",
    "            while idx < len(encodings.offset_mapping) and (encodings.offset_mapping[idx][0] != start):\n",
    "                idx += 1\n",
    "            while idx < len(encodings.offset_mapping) and (encodings.offset_mapping[idx][1] <= end):\n",
    "                labels[idx] = label\n",
    "                idx += 1\n",
    "                if idx < len(encodings.offset_mapping) and encodings.offset_mapping[idx][0] > end:\n",
    "                    break\n",
    "\n",
    "        encoded_texts.append(encodings)\n",
    "        encoded_labels.append(labels)\n",
    "\n",
    "    return encoded_texts, encoded_labels\n",
    "\n",
    "\n",
    "def encode_tags(texts, tags, max_length):\n",
    "    \"\"\" Encode tags to align with BERT tokenization. \"\"\"\n",
    "    encoded_labels = []\n",
    "    for encodings, label in zip(texts, tags):\n",
    "        labels = np.ones(len(encodings['input_ids']), dtype=int) * -100  # Ignore loss calculation for padding\n",
    "        for idx, token_id in enumerate(encodings['input_ids']):\n",
    "            if token_id != tokenizer.pad_token_id and idx < len(label):\n",
    "                labels[idx] = label[idx]\n",
    "        encoded_labels.append(labels)\n",
    "    return encoded_labels\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def create_dataset(file_path, max_length=128):\n",
    "    \"\"\" Prepare dataset for training or evaluation. \"\"\"\n",
    "    data = load_data(file_path)\n",
    "    texts, tags = encode_data(data)\n",
    "    labels = encode_tags(texts, tags, max_length)\n",
    "    return NERDataset(texts, labels)\n",
    "\n",
    "def train_and_save_model(train_dataset):\n",
    "    \"\"\" Train and save the model. \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.save_pretrained('./bert_ner_model')\n",
    "    tokenizer.save_pretrained('./bert_ner_model')\n",
    "\n",
    "def predict_and_save_results(data_file, output_file):\n",
    "    \"\"\" Predict using the model and save the results. \"\"\"\n",
    "    test_dataset = create_dataset(data_file)\n",
    "    trainer = Trainer(model=model)\n",
    "    predictions, labels, _ = trainer.predict(test_dataset)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    output_predictions = []\n",
    "    for i, (pred, label) in enumerate(zip(predictions, test_dataset.labels)):\n",
    "        true_pred = [p for (p, l) in zip(pred, label) if l != -100]\n",
    "        output_predictions.append({'id': i, 'predictions': true_pred})\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for record in output_predictions:\n",
    "            json.dump(record, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "def zip_output(output_filename, zip_filename='test.zip'):\n",
    "    \"\"\" Zip the specified file into an archive. \"\"\"\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(output_filename)\n",
    "\n",
    "# Main execution logic\n",
    "train_dataset = create_dataset('../data/train.jsonl')\n",
    "train_and_save_model(train_dataset)\n",
    "\n",
    "predict_and_save_results('../data/test.jsonl', 'test.jsonl')\n",
    "zip_output('test.jsonl', 'test.zip')\n"
   ],
   "id": "52fab443889bfabf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
